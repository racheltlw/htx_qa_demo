{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d37d10",
   "metadata": {},
   "source": [
    "## Demo for Question and Answering System (Multi) \n",
    "\n",
    "This demo will walk you through how to train a Question and Answering pipeline using Haystack for Multiple Document QA  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125d34c7",
   "metadata": {},
   "source": [
    "#### 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9adeb31a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Aug 10 15:39:46 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.36       Driver Version: 512.36       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 5000... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| N/A   53C    P8     9W /  N/A |   3282MiB / 16384MiB |      3%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      6140    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     12500      C   ...ython\\Python39\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     13432    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     16080    C+G   ...urrent\\LogiOptionsMgr.exe    N/A      |\n",
      "|    0   N/A  N/A     16108    C+G   ...e\\Current\\LogiOverlay.exe    N/A      |\n",
      "|    0   N/A  N/A     16752    C+G   ...ontend\\Docker Desktop.exe    N/A      |\n",
      "|    0   N/A  N/A     20172    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     20816    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     22288    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     24732    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     26308    C+G   ...\\atom\\app-1.58.0\\atom.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have a GPU running\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21429857",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (22.1)\n",
      "Collecting pip\n",
      "  Using cached pip-22.2.2-py3-none-any.whl (2.0 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Rachel Tan\\Documents\\France Trip\\paris_demo\\Scripts\\python.exe -m pip install --upgrade pip\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepset-ai/haystack.git\n",
      "  Cloning https://github.com/deepset-ai/haystack.git to c:\\users\\rachel tan\\appdata\\local\\temp\\pip-req-build-p_a03c61\n",
      "  Resolved https://github.com/deepset-ai/haystack.git to commit b685409c78663751ff5256b053f722cf1e08240b\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting quantulum3\n",
      "  Using cached quantulum3-0.7.10-py3-none-any.whl (10.7 MB)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.9.1-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting tika\n",
      "  Using cached tika-1.24-py3-none-any.whl\n",
      "Collecting elasticsearch<7.11,>=7.7\n",
      "  Downloading elasticsearch-7.10.1-py2.py3-none-any.whl (322 kB)\n",
      "     -------------------------------------- 322.1/322.1 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting mmh3\n",
      "  Using cached mmh3-3.0.0-cp39-cp39-win_amd64.whl (15 kB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "Requirement already satisfied: requests in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from farm-haystack==1.6.1rc0) (2.28.1)\n",
      "Collecting more-itertools\n",
      "  Downloading more_itertools-8.14.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 52.2/52.2 kB 2.6 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-2.8.5-py3-none-any.whl (2.0 MB)\n",
      "     ---------------------------------------- 2.0/2.0 MB 1.9 MB/s eta 0:00:00\n",
      "Collecting python-docx\n",
      "  Using cached python_docx-0.8.11-py3-none-any.whl\n",
      "Collecting jsonschema\n",
      "  Downloading jsonschema-4.9.1-py3-none-any.whl (79 kB)\n",
      "     ---------------------------------------- 79.5/79.5 kB 2.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from farm-haystack==1.6.1rc0) (4.64.0)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.9.0-cp39-cp39-win_amd64.whl (38.6 MB)\n",
      "     ---------------------------------------- 38.6/38.6 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting transformers==4.20.1\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "     ---------------------------------------- 4.4/4.4 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting scikit-learn>=1.0.0\n",
      "  Downloading scikit_learn-1.1.2-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 1.7 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.3-cp39-cp39-win_amd64.whl (10.6 MB)\n",
      "     ---------------------------------------- 10.6/10.6 MB 1.7 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<0.8.0,>=0.5.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.2/86.2 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting sentence-transformers>=2.2.0\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "     ---------------------------------------- 86.0/86.0 kB 2.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mlflow\n",
      "  Downloading mlflow-1.27.0-py3-none-any.whl (17.9 MB)\n",
      "     ---------------------------------------- 17.9/17.9 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: torch<1.13,>1.9 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from farm-haystack==1.6.1rc0) (1.12.1+cu113)\n",
      "Collecting seqeval\n",
      "  Using cached seqeval-1.2.2-py3-none-any.whl\n",
      "Collecting rapidfuzz<3,>=2.0.15\n",
      "  Downloading rapidfuzz-2.4.3-cp39-cp39-win_amd64.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 2.0 MB/s eta 0:00:00\n",
      "Collecting langdetect\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Collecting azure-ai-formrecognizer==3.2.0b2\n",
      "  Using cached azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n",
      "Collecting elastic-apm\n",
      "  Downloading elastic-apm-6.11.0.tar.gz (173 kB)\n",
      "     -------------------------------------- 173.8/173.8 kB 2.1 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting azure-core<1.23\n",
      "  Using cached azure_core-1.22.1-py3-none-any.whl (178 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.8/95.8 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting posthog\n",
      "  Downloading posthog-2.0.1-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack==1.6.1rc0) (1.16.0)\n",
      "Collecting msrest>=0.6.21\n",
      "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
      "     ---------------------------------------- 85.4/85.4 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting azure-common~=1.1\n",
      "  Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from transformers==4.20.1->farm-haystack==1.6.1rc0) (2022.7.25)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from transformers==4.20.1->farm-haystack==1.6.1rc0) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from transformers==4.20.1->farm-haystack==1.6.1rc0) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from transformers==4.20.1->farm-haystack==1.6.1rc0) (1.23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from transformers==4.20.1->farm-haystack==1.6.1rc0) (3.7.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from transformers==4.20.1->farm-haystack==1.6.1rc0) (6.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from elasticsearch<7.11,>=7.7->farm-haystack==1.6.1rc0) (1.26.11)\n",
      "Requirement already satisfied: certifi in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from elasticsearch<7.11,>=7.7->farm-haystack==1.6.1rc0) (2022.6.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from huggingface-hub<0.8.0,>=0.5.0->farm-haystack==1.6.1rc0) (4.3.0)\n",
      "Collecting jarowinkler<2.0.0,>=1.2.0\n",
      "  Downloading jarowinkler-1.2.0-cp39-cp39-win_amd64.whl (61 kB)\n",
      "     ---------------------------------------- 61.5/61.5 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from requests->farm-haystack==1.6.1rc0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from requests->farm-haystack==1.6.1rc0) (3.3)\n",
      "Collecting joblib>=1.0.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: torchvision in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from sentence-transformers>=2.2.0->farm-haystack==1.6.1rc0) (0.13.1+cu113)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from tqdm->farm-haystack==1.6.1rc0) (0.4.5)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.18.1-cp39-cp39-win_amd64.whl (61 kB)\n",
      "Collecting attrs>=17.4.0\n",
      "  Downloading attrs-22.1.0-py2.py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.8/58.8 kB 1.6 MB/s eta 0:00:00\n",
      "Collecting importlib-metadata!=4.7.0,>=3.7.0\n",
      "  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\n",
      "Collecting prometheus-flask-exporter\n",
      "  Downloading prometheus_flask_exporter-0.20.3-py3-none-any.whl (18 kB)\n",
      "Collecting databricks-cli>=0.8.7\n",
      "  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\n",
      "     ---------------------------------------- 81.2/81.2 kB 1.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sqlalchemy>=1.4.0\n",
      "  Downloading SQLAlchemy-1.4.40-cp39-cp39-win_amd64.whl (1.6 MB)\n",
      "     ---------------------------------------- 1.6/1.6 MB 1.3 MB/s eta 0:00:00\n",
      "Collecting cloudpickle\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from mlflow->farm-haystack==1.6.1rc0) (0.4)\n",
      "Collecting pytz\n",
      "  Using cached pytz-2022.1-py2.py3-none-any.whl (503 kB)\n",
      "Collecting sqlparse>=0.3.1\n",
      "  Using cached sqlparse-0.4.2-py3-none-any.whl (42 kB)\n",
      "Collecting Flask\n",
      "  Using cached Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "Collecting protobuf>=3.12.0\n",
      "  Using cached protobuf-4.21.5-cp39-cp39-win_amd64.whl (525 kB)\n",
      "Collecting click>=7.0\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting gitpython>=2.1.0\n",
      "  Using cached GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "Collecting querystring-parser\n",
      "  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.8.1-py3-none-any.whl (209 kB)\n",
      "     -------------------------------------- 209.8/209.8 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting waitress\n",
      "  Using cached waitress-2.1.2-py3-none-any.whl (57 kB)\n",
      "Collecting docker>=4.0.0\n",
      "  Using cached docker-5.0.3-py2.py3-none-any.whl (146 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from pandas->farm-haystack==1.6.1rc0) (2.8.2)\n",
      "Collecting backoff<2.0.0,>=1.10.0\n",
      "  Using cached backoff-1.11.1-py2.py3-none-any.whl (13 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting lxml>=2.3.2\n",
      "  Downloading lxml-4.9.1-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "     ---------------------------------------- 3.6/3.6 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting num2words\n",
      "  Downloading num2words-0.5.11-py3-none-any.whl (116 kB)\n",
      "     ------------------------------------ 116.3/116.3 kB 973.2 kB/s eta 0:00:00\n",
      "Collecting inflect\n",
      "  Downloading inflect-6.0.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from tika->farm-haystack==1.6.1rc0) (62.3.1)\n",
      "Collecting pyjwt>=1.7.0\n",
      "  Using cached PyJWT-2.4.0-py3-none-any.whl (18 kB)\n",
      "Collecting oauthlib>=3.1.0\n",
      "  Using cached oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "Collecting tabulate>=0.7.7\n",
      "  Downloading tabulate-0.8.10-py3-none-any.whl (29 kB)\n",
      "Collecting pywin32==227\n",
      "  Using cached pywin32-227-cp39-cp39-win_amd64.whl (9.1 MB)\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
      "     ---------------------------------------- 54.3/54.3 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.8.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "Collecting requests-oauthlib>=0.5.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting msrest>=0.6.21\n",
      "  Using cached msrest-0.6.21-py2.py3-none-any.whl (85 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from packaging>=20.0->transformers==4.20.1->farm-haystack==1.6.1rc0) (3.0.9)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-1.1.2-cp39-cp39-win_amd64.whl (101 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.2.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Collecting Jinja2>=3.0\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "     -------------------------------------- 232.7/232.7 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting docopt>=0.6.2\n",
      "  Using cached docopt-0.6.2-py2.py3-none-any.whl\n",
      "Collecting prometheus-client\n",
      "  Using cached prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rachel tan\\documents\\france trip\\paris_demo\\lib\\site-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack==1.6.1rc0) (9.2.0)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.1-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Building wheels for collected packages: farm-haystack, sentence-transformers, elastic-apm, databricks-cli\n",
      "  Building wheel for farm-haystack (pyproject.toml): started\n",
      "  Building wheel for farm-haystack (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for farm-haystack: filename=farm_haystack-1.6.1rc0-py3-none-any.whl size=614298 sha256=ada4ab83d7bd8092e9b7c1fb9c9ceb672f632418f6274e9c723410aad54bbb32\n",
      "  Stored in directory: C:\\Users\\Rachel Tan\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-gyy2xk7q\\wheels\\47\\fc\\da\\022dd81c208ff5db06a026f418c7060d75bda1eee7760fc577\n",
      "  Building wheel for sentence-transformers (setup.py): started\n",
      "  Building wheel for sentence-transformers (setup.py): finished with status 'done'\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=c70ebfca0fe5055ea7263e73edf44bfe2ea9c410e53b6b13fea09e432aef12ec\n",
      "  Stored in directory: c:\\users\\rachel tan\\appdata\\local\\pip\\cache\\wheels\\71\\67\\06\\162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "  Building wheel for elastic-apm (pyproject.toml): started\n",
      "  Building wheel for elastic-apm (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for elastic-apm: filename=elastic_apm-6.11.0-cp39-cp39-win_amd64.whl size=343120 sha256=1a4390e0369b9a3c3af313cdcc33e187a3c0f7453e3549c486feca340804d815\n",
      "  Stored in directory: c:\\users\\rachel tan\\appdata\\local\\pip\\cache\\wheels\\d0\\55\\bc\\2503aad9483e8cf0fa213e7a3f3adebdb9762e2319d396e2ad\n",
      "  Building wheel for databricks-cli (setup.py): started\n",
      "  Building wheel for databricks-cli (setup.py): finished with status 'done'\n",
      "  Created wheel for databricks-cli: filename=databricks_cli-0.17.0-py3-none-any.whl size=141940 sha256=bdc795a9d5979acf22bc1cd1d7fcdaaa97d986e74cafd363489709ccf5892f85\n",
      "  Stored in directory: c:\\users\\rachel tan\\appdata\\local\\pip\\cache\\wheels\\d5\\b6\\71\\c3052c82e4a88dc658dd2616b944e130c1d0ff3f77e8f02df7\n",
      "Successfully built farm-haystack sentence-transformers elastic-apm databricks-cli\n",
      "Installing collected packages: sentencepiece, pywin32, pytz, monotonic, mmh3, docopt, azure-common, zipp, websocket-client, waitress, threadpoolctl, tabulate, sqlparse, smmap, scipy, querystring-parser, pyrsistent, pyjwt, pydantic, protobuf, prometheus-client, oauthlib, num2words, networkx, more-itertools, MarkupSafe, lxml, langdetect, joblib, jarowinkler, itsdangerous, isodate, greenlet, elasticsearch, elastic-apm, dill, cloudpickle, click, backoff, attrs, Werkzeug, tika, sqlalchemy, scikit-learn, requests-oauthlib, rapidfuzz, python-docx, posthog, pandas, nltk, Mako, jsonschema, Jinja2, inflect, importlib-metadata, huggingface-hub, gitdb, docker, databricks-cli, azure-core, transformers, seqeval, quantulum3, msrest, gitpython, Flask, alembic, sentence-transformers, prometheus-flask-exporter, azure-ai-formrecognizer, mlflow, farm-haystack\n",
      "  Attempting uninstall: pywin32\n",
      "    Found existing installation: pywin32 304\n",
      "    Uninstalling pywin32-304:\n",
      "      Successfully uninstalled pywin32-304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/deepset-ai/haystack.git 'C:\\Users\\Rachel Tan\\AppData\\Local\\Temp\\pip-req-build-p_a03c61'\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Rachel Tan\\\\Documents\\\\France Trip\\\\paris_demo\\\\Lib\\\\site-packages\\\\~ywin32_system32\\\\pythoncom39.dll'\n",
      "Check the permissions.\n",
      "\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "#Install the latest master of Haystack\n",
    "#run this in command line \n",
    "!pip install git+https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc693de",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13654e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "from haystack.document_stores import InMemoryDocumentStore\n",
    "from haystack.nodes import PreProcessor, DensePassageRetriever\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.nodes import FARMReader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa936a",
   "metadata": {},
   "source": [
    "#### 2. Create a document store\n",
    "\n",
    "Think of this as a database where your documents will be stored, to be used by the QA system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59855ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore() \n",
    "#you cam also use a faiss document store which is optimised vector storage for DPR, for simplicity sake we will use InMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d02caa5",
   "metadata": {},
   "source": [
    "#### 3. Load and format your data \n",
    "The data is a mixture of news articles and covid 19 related information. We will see how a haystack retriever can filter for the most relevant articles to a question, before using a QA model to extract the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "747df368",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data as a pandas dataframe\n",
    "df = pd.read_csv('multi_demo.csv') #load into the correct format for the haystack pipeline\n",
    "#load reader and retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7cf76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reformat data so that haystack framework can use it\n",
    "def get_docs(input_df):\n",
    "    docs = []\n",
    "    for i in range(len(input_df)): \n",
    "        doc = {\n",
    "            'content': input_df['text'][i], \n",
    "            'meta': {'link': input_df['link'][i], \n",
    "                    'source': input_df['source'][i]}\n",
    "        }\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0875094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some articles are quite long so we need to split them into smaller chunks\n",
    "preprocessor = PreProcessor(split_by = 'word', \n",
    "                            split_length = 300, #each chunk is 300 words long\n",
    "                            split_overlap = 30, #each chunk overlaps with the previous chunk by 30 words\n",
    "                            split_respect_sentence_boundary= True) #will split according to complete sentences \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "873f9068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|█████████████████████████████████████████████████████████████| 268/268 [00:00<00:00, 4786.41docs/s]\n"
     ]
    }
   ],
   "source": [
    "data = get_docs(df)\n",
    "preprocessed_data = preprocessor.process(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9056d48b",
   "metadata": {},
   "source": [
    "Write our data into the document store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.write_documents(preprocessed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe77c70b",
   "metadata": {},
   "source": [
    "#### 4. Load DPR and QA Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc3837a",
   "metadata": {},
   "source": [
    "Load the DPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c24df17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
      "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
      "Updating Embedding:   0%|                                                                   | 0/322 [00:00<?, ? docs/s]\n",
      "Create embeddings:   0%|                                                                    | 0/336 [00:00<?, ? Docs/s]\u001b[A\n",
      "Create embeddings:   5%|██▊                                                        | 16/336 [00:00<00:11, 27.05 Docs/s]\u001b[A\n",
      "Create embeddings:  10%|█████▌                                                     | 32/336 [00:00<00:08, 36.07 Docs/s]\u001b[A\n",
      "Create embeddings:  14%|████████▍                                                  | 48/336 [00:01<00:07, 40.22 Docs/s]\u001b[A\n",
      "Create embeddings:  19%|███████████▏                                               | 64/336 [00:01<00:06, 42.37 Docs/s]\u001b[A\n",
      "Create embeddings:  24%|██████████████                                             | 80/336 [00:01<00:05, 44.08 Docs/s]\u001b[A\n",
      "Create embeddings:  29%|████████████████▊                                          | 96/336 [00:02<00:05, 44.73 Docs/s]\u001b[A\n",
      "Create embeddings:  33%|███████████████████▎                                      | 112/336 [00:02<00:04, 45.41 Docs/s]\u001b[A\n",
      "Create embeddings:  38%|██████████████████████                                    | 128/336 [00:03<00:04, 45.34 Docs/s]\u001b[A\n",
      "Create embeddings:  43%|████████████████████████▊                                 | 144/336 [00:03<00:04, 45.52 Docs/s]\u001b[A\n",
      "Create embeddings:  48%|███████████████████████████▌                              | 160/336 [00:03<00:03, 45.93 Docs/s]\u001b[A\n",
      "Create embeddings:  52%|██████████████████████████████▍                           | 176/336 [00:04<00:03, 46.20 Docs/s]\u001b[A\n",
      "Create embeddings:  57%|█████████████████████████████████▏                        | 192/336 [00:04<00:03, 46.22 Docs/s]\u001b[A\n",
      "Create embeddings:  62%|███████████████████████████████████▉                      | 208/336 [00:04<00:02, 46.29 Docs/s]\u001b[A\n",
      "Create embeddings:  67%|██████████████████████████████████████▋                   | 224/336 [00:05<00:02, 46.31 Docs/s]\u001b[A\n",
      "Create embeddings:  71%|█████████████████████████████████████████▍                | 240/336 [00:05<00:02, 46.53 Docs/s]\u001b[A\n",
      "Create embeddings:  76%|████████████████████████████████████████████▏             | 256/336 [00:05<00:01, 46.02 Docs/s]\u001b[A\n",
      "Create embeddings:  81%|██████████████████████████████████████████████▉           | 272/336 [00:06<00:01, 45.91 Docs/s]\u001b[A\n",
      "Create embeddings:  86%|█████████████████████████████████████████████████▋        | 288/336 [00:06<00:01, 45.52 Docs/s]\u001b[A\n",
      "Create embeddings:  90%|████████████████████████████████████████████████████▍     | 304/336 [00:06<00:00, 45.86 Docs/s]\u001b[A\n",
      "Create embeddings:  95%|███████████████████████████████████████████████████████▏  | 320/336 [00:07<00:00, 45.66 Docs/s]\u001b[A\n",
      "Documents Processed: 10000 docs [00:07, 1336.79 docs/s]                                                                \u001b[A\n"
     ]
    }
   ],
   "source": [
    "retriever = DensePassageRetriever(\n",
    "    document_store=document_store,\n",
    "    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n",
    "    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n",
    "    max_seq_len_query=128, \n",
    "    max_seq_len_passage=512,\n",
    "    batch_size=16,\n",
    "    use_gpu=True, #if you do not have a gpu you can turn this off, it will just take longer\n",
    ")\n",
    "\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b2f3c",
   "metadata": {},
   "source": [
    "Load the Reader (this is the QA model from Huggingface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0521e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bc6c9d",
   "metadata": {},
   "source": [
    "Create a pipeline using both the retriever and reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "44b7fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9c9cea",
   "metadata": {},
   "source": [
    "#### 5. Trying out our pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d2e6cc",
   "metadata": {},
   "source": [
    "Here is a simple function to allow us to display our results nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9cda3712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_preds_df(results):\n",
    "    answers = results[\"answers\"]\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    keys_to_keep = set([\"answer\", \"context\", \"score\", \"probability\"])\n",
    "\n",
    "    # filter the results\n",
    "    filtered_answers = []\n",
    "    for ans in answers:\n",
    "        filtered_answers.append({'answer': ans.answer, 'context': ans.context, 'score': ans.score, \n",
    "                               'link': ans.meta['link'], 'source': ans.meta['source']})\n",
    "\n",
    "    df_res = pd.DataFrame({\"answer\":[], \"context\":[], \"score\":[], \"link\":[], \"source\":[]})\n",
    "\n",
    "    for i in filtered_answers:\n",
    "        df_res.loc[len(df_res)] = i\n",
    "\n",
    "    df_res.sort_values(by=['score'], inplace = True, ascending=False)\n",
    "    df_res = df_res.reset_index(drop = True)\n",
    "    df_res['score'] = df_res['score'].round(2)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66effcfa",
   "metadata": {},
   "source": [
    "Run the pipeline on a question \n",
    "- The retriever filters out the top 20 most relevant articles\n",
    "- Then the QA finds the top 5 most probable answers from those articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b9cb96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.37 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  9.12 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.23 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.62 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.29 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 22.77 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 33.04 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 45.52 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 55.60 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 55.61 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 52.63 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.81 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.81 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.01 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.56 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.94 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.28 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.84 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 55.27 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 54.90 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wuhan China</td>\n",
       "      <td>t is causing the 2019 novel coronavirus outbreak, first identified in Wuhan ...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>\\nhttps://www.cdc.gov/coronavirus/2019-ncov/faq.html</td>\n",
       "      <td>Center for Disease Control and Prevention (CDC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>The novel coronavirus detected in China is genetically closely related to th...</td>\n",
       "      <td>0.85</td>\n",
       "      <td>https://www.ecdc.europa.eu/en/novel-coronavirus-china/questions-answers</td>\n",
       "      <td>European Centre for Disease Prevention and Control (ECDC)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>animals</td>\n",
       "      <td>Coronaviruses are a large family of viruses that are common in animals. Occa...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>https://www.who.int/news-room/q-a-detail/q-a-coronaviruses</td>\n",
       "      <td>World Health Organization (WHO)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>humans</td>\n",
       "      <td>ily of viruses. There are some coronaviruses that commonly circulate in huma...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/Immunization/nCoV2019.aspx#</td>\n",
       "      <td>California Department of Public Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wuhan City</td>\n",
       "      <td>This virus was first detected in Wuhan City, Hubei Province, China. The firs...</td>\n",
       "      <td>0.70</td>\n",
       "      <td>\\nhttps://www.cdc.gov/coronavirus/2019-ncov/faq.html</td>\n",
       "      <td>Center for Disease Control and Prevention (CDC)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        answer  \\\n",
       "0  Wuhan China   \n",
       "1        China   \n",
       "2      animals   \n",
       "3       humans   \n",
       "4   Wuhan City   \n",
       "\n",
       "                                                                           context  \\\n",
       "0  t is causing the 2019 novel coronavirus outbreak, first identified in Wuhan ...   \n",
       "1  The novel coronavirus detected in China is genetically closely related to th...   \n",
       "2  Coronaviruses are a large family of viruses that are common in animals. Occa...   \n",
       "3  ily of viruses. There are some coronaviruses that commonly circulate in huma...   \n",
       "4  This virus was first detected in Wuhan City, Hubei Province, China. The firs...   \n",
       "\n",
       "   score  \\\n",
       "0   0.97   \n",
       "1   0.85   \n",
       "2   0.77   \n",
       "3   0.73   \n",
       "4   0.70   \n",
       "\n",
       "                                                                          link  \\\n",
       "0                         \\nhttps://www.cdc.gov/coronavirus/2019-ncov/faq.html   \n",
       "1      https://www.ecdc.europa.eu/en/novel-coronavirus-china/questions-answers   \n",
       "2                   https://www.who.int/news-room/q-a-detail/q-a-coronaviruses   \n",
       "3  https://www.cdph.ca.gov/Programs/CID/DCDC/Pages/Immunization/nCoV2019.aspx#   \n",
       "4                         \\nhttps://www.cdc.gov/coronavirus/2019-ncov/faq.html   \n",
       "\n",
       "                                                      source  \n",
       "0            Center for Disease Control and Prevention (CDC)  \n",
       "1  European Centre for Disease Prevention and Control (ECDC)  \n",
       "2                            World Health Organization (WHO)  \n",
       "3                     California Department of Public Health  \n",
       "4            Center for Disease Control and Prevention (CDC)  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn = 'Where did the coronavirus first appear? '\n",
    "prediction = pipeline.run(query=qn, params={'Retriever': {'top_k': 20}, 'Reader': {'top_k':5}})\n",
    "prediction_df = print_preds_df(prediction)\n",
    "prediction_df #shows the top 5 answers by score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1dc2d3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.73 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 20.61 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 13.58 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 15.69 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.03 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.08 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.46 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 26.31 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 63.00 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.48 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 27.02 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 38.60 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.50 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.87 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 63.43 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 62.51 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 41.03 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 42.96 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 58.83 Batches/s]\n",
      "Inferencing Samples: 100%|█████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 37.03 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>an attack</td>\n",
       "      <td>One of the victims of an attack at Marina Bay Sands on Halloween sustained b...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>https://www.asiaone.com/singapore/halloween-horror-attack-marina-bay-sands-v...</td>\n",
       "      <td>Asia One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accosted</td>\n",
       "      <td>6am after a Halloween-themed party at nightclub Marquee when they were accos...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>https://www.asiaone.com/singapore/halloween-horror-attack-marina-bay-sands-v...</td>\n",
       "      <td>Asia One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>attacked</td>\n",
       "      <td>red Hello Kitty theme. Banquet waiter Joshua Koh Kian Yong (above) was attac...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>https://www.asiaone.com/singapore/businessman-gets-6-years-jail-paying-hitme...</td>\n",
       "      <td>Asia One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disappearance from the restaurant</td>\n",
       "      <td>Singh, said her husband was behaving suspiciously and his disappearance fro...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>https://www.asiaone.com/singapore/woman-jailed-7-months-smashing-beer-bottle...</td>\n",
       "      <td>Asia One</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>after the accident</td>\n",
       "      <td>Lee, who was seated in the dock, as the driver. DPP Koh said that after the ...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>https://www.asiaone.com/singapore/man-denies-driving-maserati-dragged-traffi...</td>\n",
       "      <td>Asia One</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              answer  \\\n",
       "0                          an attack   \n",
       "1                           accosted   \n",
       "2                           attacked   \n",
       "3  disappearance from the restaurant   \n",
       "4                 after the accident   \n",
       "\n",
       "                                                                           context  \\\n",
       "0  One of the victims of an attack at Marina Bay Sands on Halloween sustained b...   \n",
       "1  6am after a Halloween-themed party at nightclub Marquee when they were accos...   \n",
       "2  red Hello Kitty theme. Banquet waiter Joshua Koh Kian Yong (above) was attac...   \n",
       "3   Singh, said her husband was behaving suspiciously and his disappearance fro...   \n",
       "4  Lee, who was seated in the dock, as the driver. DPP Koh said that after the ...   \n",
       "\n",
       "   score  \\\n",
       "0   0.33   \n",
       "1   0.21   \n",
       "2   0.11   \n",
       "3   0.05   \n",
       "4   0.05   \n",
       "\n",
       "                                                                              link  \\\n",
       "0  https://www.asiaone.com/singapore/halloween-horror-attack-marina-bay-sands-v...   \n",
       "1  https://www.asiaone.com/singapore/halloween-horror-attack-marina-bay-sands-v...   \n",
       "2  https://www.asiaone.com/singapore/businessman-gets-6-years-jail-paying-hitme...   \n",
       "3  https://www.asiaone.com/singapore/woman-jailed-7-months-smashing-beer-bottle...   \n",
       "4  https://www.asiaone.com/singapore/man-denies-driving-maserati-dragged-traffi...   \n",
       "\n",
       "     source  \n",
       "0  Asia One  \n",
       "1  Asia One  \n",
       "2  Asia One  \n",
       "3  Asia One  \n",
       "4  Asia One  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qn = \"What happened on Halloween at Marina Bay Sands?\"\n",
    "prediction = pipeline.run(query=qn, params={'Retriever': {'top_k': 20}, 'Reader': {'top_k':5}})\n",
    "prediction_df = print_preds_df(prediction)\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa1eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paris_demo",
   "language": "python",
   "name": "paris_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
